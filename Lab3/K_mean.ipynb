{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Invalid alias: The name clear can't be aliased because it is another magic command.\n",
      "ERROR:root:Invalid alias: The name more can't be aliased because it is another magic command.\n",
      "ERROR:root:Invalid alias: The name less can't be aliased because it is another magic command.\n",
      "ERROR:root:Invalid alias: The name man can't be aliased because it is another magic command.\n",
      "/home/ahasan/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________________  Instance: 0 _____________________\n",
      "Iteration 1 Error:  242.44607232911028\n",
      "Iteration 2 Error:  192.01708510030656\n",
      "Iteration 3 Error:  185.02162686886473\n",
      "Iteration 4 Error:  180.82939477960542\n",
      "Iteration 5 Error:  178.56793566940675\n",
      "Iteration 6 Error:  177.07603756109847\n",
      "Iteration 7 Error:  176.25990792295096\n",
      "Iteration 8 Error:  176.0075160130971\n",
      "Iteration 9 Error:  175.95105111983136\n",
      "Iteration 10 Error:  175.94874121431866\n",
      "Average of Variance and Silhouette Coefficient\n",
      "175.94874121431866,0.2491465574817626\n",
      "BCubed Precision : 0.5911898015663225 , Recall : 0.49788771898212697\n",
      "BCubed precision: 0.5911898015663225 BCubed Recall: 0.49788771898212697\n",
      "_______________________  Instance: 1 _____________________\n",
      "Iteration 1 Error:  253.09947397054472\n",
      "Iteration 2 Error:  178.43556098686878\n",
      "Iteration 3 Error:  176.83882054611777\n",
      "Iteration 4 Error:  176.19090899077412\n",
      "Iteration 5 Error:  176.12751971562827\n",
      "Iteration 6 Error:  176.04467681387374\n",
      "Iteration 7 Error:  175.97334443639346\n",
      "Iteration 8 Error:  175.94874121431866\n",
      "Average of Variance and Silhouette Coefficient\n",
      "175.94874121431866,0.2491465574817626\n",
      "BCubed Precision : 0.5911898015663225 , Recall : 0.49788771898212697\n",
      "BCubed precision: 0.5911898015663225 BCubed Recall: 0.49788771898212697\n",
      "_______________________  Instance: 2 _____________________\n",
      "Iteration 1 Error:  263.3288823827864\n",
      "Iteration 2 Error:  184.7556498319895\n",
      "Iteration 3 Error:  180.56381686078424\n",
      "Iteration 4 Error:  178.4474488978493\n",
      "Iteration 5 Error:  176.9826346810764\n",
      "Iteration 6 Error:  176.16194067216844\n",
      "Iteration 7 Error:  175.9902273490728\n",
      "Iteration 8 Error:  175.95105111983136\n",
      "Iteration 9 Error:  175.94874121431866\n",
      "Average of Variance and Silhouette Coefficient\n",
      "175.94874121431866,0.2491465574817626\n",
      "BCubed Precision : 0.5911898015663225 , Recall : 0.49788771898212697\n",
      "BCubed precision: 0.5911898015663225 BCubed Recall: 0.49788771898212697\n",
      "_______________________  Instance: 3 _____________________\n",
      "Iteration 1 Error:  250.85505039501916\n",
      "Iteration 2 Error:  184.5581126489026\n",
      "Iteration 3 Error:  180.9369139235396\n",
      "Iteration 4 Error:  178.6578532970525\n",
      "Iteration 5 Error:  177.4091598015163\n",
      "Iteration 6 Error:  176.6028723225924\n",
      "Iteration 7 Error:  176.293674185767\n",
      "Iteration 8 Error:  176.1631712449843\n",
      "Iteration 9 Error:  176.04716429642966\n",
      "Iteration 10 Error:  175.9628323653252\n",
      "Iteration 11 Error:  175.94874121431866\n",
      "Average of Variance and Silhouette Coefficient\n",
      "175.94874121431866,0.2491465574817626\n",
      "BCubed Precision : 0.5911898015663225 , Recall : 0.49788771898212697\n",
      "BCubed precision: 0.5911898015663225 BCubed Recall: 0.49788771898212697\n",
      "_______________________  Instance: 4 _____________________\n",
      "Iteration 1 Error:  203.52509475264503\n",
      "Iteration 2 Error:  176.54714924636295\n",
      "Iteration 3 Error:  175.74159817194823\n",
      "Iteration 4 Error:  175.71184106674295\n",
      "Iteration 5 Error:  175.82525038804383\n",
      "Iteration 6 Error:  175.89630374014806\n",
      "Iteration 7 Error:  175.95068559815184\n",
      "Average of Variance and Silhouette Coefficient\n",
      "175.9491300910853,0.24904312037955642\n",
      "BCubed Precision : 0.5911712543836903 , Recall : 0.4983981568548832\n",
      "BCubed precision: 0.5911860921297961 BCubed Recall: 0.49798980655667824\n",
      "Average of Variance and Silhouette Coefficient over 5 iteration\n",
      "175.9491300910853,0.24904312037955642\n",
      "BCubed precision: 0.5911860921297961 BCubed Recall: 0.49798980655667824\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import  silhouette_score\n",
    "from statistics import mean \n",
    "from matplotlib import pyplot as plt\n",
    "# random.seed(5)\n",
    "\n",
    "\n",
    "\n",
    "def getData(dataset_name):\n",
    "    attribute_file_name = 'Data/'+dataset_name+\".attribute\"\n",
    "    dataset_file_name = 'Data/'+dataset_name+\".data\"\n",
    "    att = pd.read_csv(attribute_file_name,\n",
    "                      delim_whitespace=True,\n",
    "                     header = None)\n",
    "    attributes = {rows[0]:rows[1] for _,rows in att.iterrows()}\n",
    "    dataset = pd.read_csv(dataset_file_name,\n",
    "                      names=list(attributes.keys()))\n",
    "    if 'class' in attributes: \n",
    "        tuple_labels = []\n",
    "        label_count = []\n",
    "        classes = {}\n",
    "        tuple_labels = list(dataset[\"class\"])\n",
    "        classes_unique, label_count = np.unique(tuple_labels, return_counts=True)\n",
    "        idx = 0\n",
    "        for class_ in classes_unique:\n",
    "            classes[class_] = idx\n",
    "            idx += 1\n",
    "#         print(classes)\n",
    "        del attributes['class']; del dataset['class']\n",
    "    return  dataset,classes,tuple_labels,label_count\n",
    "\n",
    "\n",
    "\n",
    "def dist(x1,x2, minkowski = 2):\n",
    "    val = 0.0\n",
    "#     print(\"In error: \",attributes)\n",
    "    for att in dataset.columns:\n",
    "#         if attributes[att]=='value':\n",
    "        val += (x1[att] - x2[att])*(x1[att] - x2[att])\n",
    "    val = math.sqrt(val)\n",
    "    return val\n",
    "\n",
    "def calculateError():\n",
    "    error = 0\n",
    "    sqrt_error = 0\n",
    "    quality = 0\n",
    "    for i in range(len(dataset)):\n",
    "        distance = dist(dataset.iloc[i], pd.DataFrame(centroids[cluster_index[i]],index=[0]))\n",
    "        sqrt_error += distance\n",
    "        error += distance\n",
    "        quality += (1-distance)\n",
    "#     print(\"Error:\",error,\"Squared Error:\", sqrt_error,\"Quality: \",quality)\n",
    "#     Print_Quality = quality\n",
    "    return sqrt_error,quality\n",
    "\n",
    "def assignCluster():\n",
    "    flag = False\n",
    "    for i in range(len(dataset)):\n",
    "        min_val = math.inf\n",
    "        min_idx = -1\n",
    "        for j in range(len(centroids)):\n",
    "            distance = dist(dataset.iloc[i], pd.DataFrame(centroids[j],index=[0]))\n",
    "            if distance < min_val:\n",
    "                min_val = distance\n",
    "                min_idx = j\n",
    "        if flag==False and cluster_index[i] != min_idx:\n",
    "            print\n",
    "            flag = True\n",
    "        cluster_index[i] = min_idx\n",
    "    return flag\n",
    "\n",
    "def newCentroids():\n",
    "    for i in range(len(centroids)):\n",
    "        List = [j for j in range(len(dataset)) if cluster_index[j]==i]\n",
    "#         print(\"Len:\",len(List))\n",
    "        clustered_data = dataset.iloc[List]\n",
    "        if (len(clustered_data)>0):\n",
    "            for column in dataset.columns:\n",
    "                centroids[i][column] = clustered_data[column].mean()\n",
    "\n",
    "\n",
    "def bcubed():\n",
    "    if len(classes) == 0:\n",
    "        print(\"no labels\")\n",
    "        return\n",
    "    cluster_label_combo = np.zeros([len(centroids), len(classes)])\n",
    "    cluster_count = np.zeros([len(centroids)])\n",
    "    for i in range(len(dataset)):\n",
    "        cluster_label_combo[cluster_index[i]][classes[tuple_labels[i]]] += 1.0\n",
    "        cluster_count[cluster_index[i]] += 1.0\n",
    "\n",
    "    bcp = 0.0\n",
    "    bcr = 0.0\n",
    "    for i in range(len(dataset)):\n",
    "        bcp += cluster_label_combo[cluster_index[i]][classes[tuple_labels[i]]]/cluster_count[cluster_index[i]]\n",
    "        bcr += cluster_label_combo[cluster_index[i]][classes[tuple_labels[i]]]/label_count[classes[tuple_labels[i]]]\n",
    "    bcp /= len(dataset)\n",
    "    bcr /= len(dataset)\n",
    "    print(\"BCubed Precision :\", bcp, \", Recall :\", bcr)\n",
    "    return bcp,bcr\n",
    "\n",
    "def showGraph(loop_num):\n",
    "    x = []\n",
    "    y = []\n",
    "    color = [\"red\",\"green\", \"blue\", \"yellow\", \"black\"]\n",
    "    for i in range(len(centroids)+1):\n",
    "        x.append([])\n",
    "        y.append([])\n",
    "    for i in range(len(dataset)):\n",
    "        x[cluster_index[i]].append(dataset.iloc[i]['x'])\n",
    "        y[cluster_index[i]].append(dataset.iloc[i]['y'])\n",
    "    for i in range(len(centroids)):\n",
    "        x[len(centroids)].append(centroids[i]['x'])\n",
    "        y[len(centroids)].append(centroids[i]['y'])\n",
    "    for i in range(len(centroids)):\n",
    "        plt.scatter(x[i],y[i],color=color[i])\n",
    "    print(\"Centroids: \")\n",
    "    print(x[-1],y[-1])\n",
    "    \n",
    "    plt.scatter(x[-1],y[-1],color=\"black\")\n",
    "    filename = \"fig_\"+str(loop_num)+\".png\"\n",
    "#     print(filename)\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset_name = 'ILPD'\n",
    "k = 2\n",
    "groundTruth = True\n",
    "if groundTruth:\n",
    "    dataset,classes,tuple_labels,label_count = getData(dataset_name)\n",
    "    bcp = []\n",
    "    bcr = []\n",
    "else:\n",
    "    filepath=\"Data/\"+dataset_name+\".data\"\n",
    "    dataset = pd.read_csv(filepath)\n",
    "dataset = dataset.dropna()\n",
    "min_max_scaler = MinMaxScaler()\n",
    "value_attributes = list(dataset.columns)\n",
    "dataset[value_attributes] = min_max_scaler.fit_transform(dataset[value_attributes])\n",
    "silhouette_avg=[]\n",
    "variance = []\n",
    "\n",
    "for _ in range(5):\n",
    "    print(\"_______________________  Instance:\",_,\"_____________________\")\n",
    "    centroids = []\n",
    "    cluster_index = [-1]*len(dataset)\n",
    "    # print(cluster_index)\n",
    "    for i in range (k): \n",
    "        centroid = {}\n",
    "        random_number = random.randint(0,len(dataset)-1)\n",
    "        for column in dataset.columns:\n",
    "            centroid[column] = dataset.iloc[random_number][column]\n",
    "        centroids.append(centroid)\n",
    "    loop_num = 0\n",
    "\n",
    "    while True:\n",
    "        loop_num += 1\n",
    "        \n",
    "        continue_loop = assignCluster()\n",
    "    #     print(\"Silhouette average: \", silhouette_avg)\n",
    "        newError,Print_Quality = calculateError()\n",
    "        print(\"Iteration\",loop_num,\"Error: \",newError)\n",
    "        newCentroids()\n",
    "        if continue_loop == False or loop_num>20:\n",
    "            break\n",
    "\n",
    "    silhouette_avg.append(silhouette_score(dataset.values.tolist(),cluster_index))\n",
    "    variance.append(newError)\n",
    "    print(\"Average of Variance and Silhouette Coefficient\")\n",
    "    print(str(str(mean(variance)))+\",\"+str(mean(silhouette_avg)))\n",
    "    if groundTruth:\n",
    "        precision,recall = bcubed()\n",
    "        bcp.append(precision); bcr.append(recall)\n",
    "        print(\"BCubed precision:\",mean(bcp),\"BCubed Recall:\",mean(bcr))\n",
    "\n",
    "print(\"Average of Variance and Silhouette Coefficient over 5 iteration\")\n",
    "print(str(str(mean(variance)))+\",\"+str(mean(silhouette_avg)))\n",
    "if groundTruth:\n",
    "    print(\"BCubed precision:\",mean(bcp),\"BCubed Recall:\",mean(bcr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Area  Perimeter  Compactness  L_kernel  W_kernel  Coefficient  \\\n",
      "14.88  0.446281   0.662432     0.368806  0.501069  0.033497     0.215165   \n",
      "14.29  0.347107   0.879310     0.220721  0.503920  0.256149     0.150665   \n",
      "16.14  0.533058   0.864791     0.427365  0.664291  0.078133     0.322994   \n",
      "14.03  0.361570   0.648820     0.303491  0.406985  0.126081     0.237322   \n",
      "\n",
      "       L_groove  \n",
      "14.88       0.0  \n",
      "14.29       0.0  \n",
      "16.14       0.0  \n",
      "14.03       0.0  \n"
     ]
    }
   ],
   "source": [
    "print(dataset.iloc[[1,2,4,10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(math.inf>=100000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Area           0.630165\n",
       "Perimeter      0.604356\n",
       "Compactness    0.649775\n",
       "L_kernel       0.595153\n",
       "W_kernel       0.168863\n",
       "Coefficient    0.668636\n",
       "L_groove       0.000000\n",
       "Name: 16.63, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.iloc[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "9\n",
      "21\n",
      "10\n",
      "9\n",
      "5\n",
      "2\n",
      "20\n",
      "4\n",
      "9\n",
      "15\n",
      "5\n",
      "1\n",
      "2\n",
      "19\n",
      "17\n",
      "12\n",
      "1\n",
      "7\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(random.randint(0,21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Invalid alias: The name clear can't be aliased because it is another magic command.\n",
      "ERROR:root:Invalid alias: The name more can't be aliased because it is another magic command.\n",
      "ERROR:root:Invalid alias: The name less can't be aliased because it is another magic command.\n",
      "ERROR:root:Invalid alias: The name man can't be aliased because it is another magic command.\n",
      "/home/ahasan/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,175.99434121225957\n",
      "3,161.91761890995173\n",
      "4,157.7101566411889\n",
      "5,146.10299564113262\n",
      "6,133.64309040453176\n",
      "7,129.8011272608027\n",
      "8,127.1044169440692\n",
      "9,122.689393439627\n",
      "10,121.45494427640085\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import  silhouette_score\n",
    "from statistics import mean \n",
    "from matplotlib import pyplot as plt\n",
    "# random.seed(5)\n",
    "\n",
    "\n",
    "\n",
    "def getData(dataset_name):\n",
    "    attribute_file_name = 'Data/'+dataset_name+\".attribute\"\n",
    "    dataset_file_name = 'Data/'+dataset_name+\".data\"\n",
    "    att = pd.read_csv(attribute_file_name,\n",
    "                      delim_whitespace=True,\n",
    "                     header = None)\n",
    "    attributes = {rows[0]:rows[1] for _,rows in att.iterrows()}\n",
    "    dataset = pd.read_csv(dataset_file_name,\n",
    "                      names=list(attributes.keys()))\n",
    "    if 'class' in attributes: \n",
    "        tuple_labels = []\n",
    "        label_count = []\n",
    "        classes = {}\n",
    "        tuple_labels = list(dataset[\"class\"])\n",
    "        classes_unique, label_count = np.unique(tuple_labels, return_counts=True)\n",
    "        idx = 0\n",
    "        for class_ in classes_unique:\n",
    "            classes[class_] = idx\n",
    "            idx += 1\n",
    "#         print(classes)\n",
    "        del attributes['class']; del dataset['class']\n",
    "    return  dataset,classes,tuple_labels,label_count\n",
    "\n",
    "\n",
    "\n",
    "def dist(x1,x2, minkowski = 2):\n",
    "    val = 0.0\n",
    "#     print(\"In error: \",attributes)\n",
    "    for att in dataset.columns:\n",
    "#         if attributes[att]=='value':\n",
    "        val += (x1[att] - x2[att])*(x1[att] - x2[att])\n",
    "    val = math.sqrt(val)\n",
    "    return val\n",
    "\n",
    "def calculateError():\n",
    "    error = 0\n",
    "    sqrt_error = 0\n",
    "    quality = 0\n",
    "    for i in range(len(dataset)):\n",
    "        distance = dist(dataset.iloc[i], pd.DataFrame(centroids[cluster_index[i]],index=[0]))\n",
    "        sqrt_error += distance\n",
    "        error += distance\n",
    "        quality += (1-distance)\n",
    "#     print(\"Error:\",error,\"Squared Error:\", sqrt_error,\"Quality: \",quality)\n",
    "#     Print_Quality = quality\n",
    "    return sqrt_error,quality\n",
    "\n",
    "def assignCluster():\n",
    "    flag = False\n",
    "    for i in range(len(dataset)):\n",
    "        min_val = math.inf\n",
    "        min_idx = -1\n",
    "        for j in range(len(centroids)):\n",
    "            distance = dist(dataset.iloc[i], pd.DataFrame(centroids[j],index=[0]))\n",
    "            if distance < min_val:\n",
    "                min_val = distance\n",
    "                min_idx = j\n",
    "        if flag==False and cluster_index[i] != min_idx:\n",
    "            print\n",
    "            flag = True\n",
    "        cluster_index[i] = min_idx\n",
    "    return flag\n",
    "\n",
    "def newCentroids():\n",
    "    for i in range(len(centroids)):\n",
    "        List = [j for j in range(len(dataset)) if cluster_index[j]==i]\n",
    "#         print(\"Len:\",len(List))\n",
    "        clustered_data = dataset.iloc[List]\n",
    "        if (len(clustered_data)>0):\n",
    "            for column in dataset.columns:\n",
    "                centroids[i][column] = clustered_data[column].mean()\n",
    "\n",
    "\n",
    "def bcubed():\n",
    "    if len(classes) == 0:\n",
    "        print(\"no labels\")\n",
    "        return\n",
    "    cluster_label_combo = np.zeros([len(centroids), len(classes)])\n",
    "    cluster_count = np.zeros([len(centroids)])\n",
    "    for i in range(len(dataset)):\n",
    "        cluster_label_combo[cluster_index[i]][classes[tuple_labels[i]]] += 1.0\n",
    "        cluster_count[cluster_index[i]] += 1.0\n",
    "\n",
    "    bcp = 0.0\n",
    "    bcr = 0.0\n",
    "    for i in range(len(dataset)):\n",
    "        bcp += cluster_label_combo[cluster_index[i]][classes[tuple_labels[i]]]/cluster_count[cluster_index[i]]\n",
    "        bcr += cluster_label_combo[cluster_index[i]][classes[tuple_labels[i]]]/label_count[classes[tuple_labels[i]]]\n",
    "    bcp /= len(dataset)\n",
    "    bcr /= len(dataset)\n",
    "    print(\"BCubed Precision :\", bcp, \", Recall :\", bcr)\n",
    "    return bcp,bcr\n",
    "\n",
    "def showGraph(loop_num):\n",
    "    x = []\n",
    "    y = []\n",
    "    color = [\"red\",\"green\", \"blue\", \"yellow\", \"black\"]\n",
    "    for i in range(len(centroids)+1):\n",
    "        x.append([])\n",
    "        y.append([])\n",
    "    for i in range(len(dataset)):\n",
    "        x[cluster_index[i]].append(dataset.iloc[i]['x'])\n",
    "        y[cluster_index[i]].append(dataset.iloc[i]['y'])\n",
    "    for i in range(len(centroids)):\n",
    "        x[len(centroids)].append(centroids[i]['x'])\n",
    "        y[len(centroids)].append(centroids[i]['y'])\n",
    "    for i in range(len(centroids)):\n",
    "        plt.scatter(x[i],y[i],color=color[i])\n",
    "    print(\"Centroids: \")\n",
    "    print(x[-1],y[-1])\n",
    "    \n",
    "    plt.scatter(x[-1],y[-1],color=\"black\")\n",
    "    filename = \"fig_\"+str(loop_num)+\".png\"\n",
    "#     print(filename)\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset_name = 'ILPD'\n",
    "k = 3\n",
    "groundTruth = True\n",
    "if groundTruth:\n",
    "    dataset,classes,tuple_labels,label_count = getData(dataset_name)\n",
    "    bcp = []\n",
    "    bcr = []\n",
    "else:\n",
    "    filepath=\"Data/\"+dataset_name+\".data\"\n",
    "    dataset = pd.read_csv(filepath)\n",
    "dataset = dataset.dropna()\n",
    "min_max_scaler = MinMaxScaler()\n",
    "value_attributes = list(dataset.columns)\n",
    "dataset[value_attributes] = min_max_scaler.fit_transform(dataset[value_attributes])\n",
    "silhouette_avg=[]\n",
    "variance = []\n",
    "\n",
    "for _ in range(2,11):\n",
    "    k = _\n",
    "#     print(\"_______________________  Instance:\",_,\"_____________________\")\n",
    "    centroids = []\n",
    "    cluster_index = [-1]*len(dataset)\n",
    "    # print(cluster_index)\n",
    "    for i in range (k): \n",
    "        centroid = {}\n",
    "        random_number = random.randint(0,len(dataset)-1)\n",
    "        for column in dataset.columns:\n",
    "            centroid[column] = dataset.iloc[random_number][column]\n",
    "        centroids.append(centroid)\n",
    "    loop_num = 0\n",
    "\n",
    "    while True:\n",
    "        loop_num += 1\n",
    "        \n",
    "        continue_loop = assignCluster()\n",
    "    #     print(\"Silhouette average: \", silhouette_avg)\n",
    "        newError,Print_Quality = calculateError()\n",
    "#         print(\"Iteration\",loop_num,\"Error: \",newError)\n",
    "        newCentroids()\n",
    "        if continue_loop == False or loop_num>20:\n",
    "            break\n",
    "\n",
    "    print(str(k)+\",\"+str(newError))\n",
    "#     silhouette_avg.append(silhouette_score(dataset.values.tolist(),cluster_index))\n",
    "#     variance.append(newError)\n",
    "#     print(\"Average of Variance and Silhouette Coefficient\")\n",
    "#     print(str(str(mean(variance)))+\",\"+str(mean(silhouette_avg)))\n",
    "#     if groundTruth:\n",
    "#         precision,recall = bcubed()\n",
    "#         bcp.append(precision); bcr.append(recall)\n",
    "#         print(\"BCubed precision:\",mean(bcp),\"BCubed Recall:\",mean(bcr))\n",
    "\n",
    "# print(\"Average of Variance and Silhouette Coefficient over 5 iteration\")\n",
    "# print(str(str(mean(variance)))+\",\"+str(mean(silhouette_avg)))\n",
    "# if groundTruth:\n",
    "#     print(\"BCubed precision:\",mean(bcp),\"BCubed Recall:\",mean(bcr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
