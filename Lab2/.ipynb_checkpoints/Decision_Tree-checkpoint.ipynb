{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5ddc40cc01e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mID3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0mprintDecisionTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-5ddc40cc01e0>\u001b[0m in \u001b[0;36mID3\u001b[0;34m(data, features, target_attribute_name, parent_node_class)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0msplit_point\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0mGR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInfoGainRatio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_attribute_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m             \u001b[0mitem_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0msplit_point\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-5ddc40cc01e0>\u001b[0m in \u001b[0;36mInfoGainRatio\u001b[0;34m(data, split_attribute_name, split_att_type, target_name)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mtotal_entropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0msplit_att_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'category'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;31m#         vals,counts= np.unique(data[split_attribute_name],return_counts=True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"check\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def getData(dataset_name):\n",
    "    attribute_file_name = 'Data/'+dataset_name+\".attribute\"\n",
    "    dataset_file_name = 'Data/'+dataset_name+\".data\"\n",
    "    att = pd.read_csv(attribute_file_name, \n",
    "                      delim_whitespace=True,\n",
    "                     header = None)\n",
    "    attributes = {rows[0]:rows[1] for _,rows in att.iterrows()}\n",
    "    dataset = pd.read_csv(dataset_file_name,\n",
    "                      names=list(attributes.keys()))\n",
    "    return attributes, dataset\n",
    "\n",
    "\n",
    "def entropy(target_col, col_type, split_point=0.0):\n",
    "    if col_type == 'category':\n",
    "        counts = list(data[split_attribute_name].value_counts().values)\n",
    "#         _,counts = np.unique(target_col,return_counts = True)\n",
    "    else:\n",
    "        left = target_col <= split_point\n",
    "        right = target_col > split_point\n",
    "        # print(left, right)\n",
    "        counts = [len(target_col[left]), len(target_col[right])]\n",
    "    # print(split_point, counts)\n",
    "    entropy = np.sum([(-counts[i]/np.sum(counts))*np.log2(counts[i]/np.sum(counts)) for i in range(len(counts))])\n",
    "    return entropy\n",
    "\n",
    "def InfoGainRatio(data,split_attribute_name,split_att_type,target_name=\"class\"):\n",
    "    total_entropy = entropy(data[target_name], attributes[target_name])\n",
    "\n",
    "    if split_att_type == 'category':\n",
    "#         vals,counts= np.unique(data[split_attribute_name],return_counts=True\n",
    "        print(\"check\")\n",
    "        tmp = data[split_attribute_name].value_counts()\n",
    "        val = list(tmp.index)\n",
    "        counts = list(tmp.values)\n",
    "        \n",
    "        information = data[split_att_type].value_counts()\n",
    "\n",
    "        Weighted_Entropy = np.sum([(counts[i]/np.sum(counts))*entropy(data.where(data[split_attribute_name]==vals[i]).dropna()[target_name], attributes[split_attribute_name]) for i in range(len(vals))])\n",
    "        Information_Gain = total_entropy - Weighted_Entropy\n",
    "        \n",
    "        if Information_Gain == 0.0:\n",
    "            return Information_Gain\n",
    "        Gain_Ratio = Information_Gain/entropy(data[split_attribute_name], attributes[split_attribute_name])\n",
    "        return Gain_Ratio, None\n",
    "    else:\n",
    "        values = list(np.unique(data[split_attribute_name]))\n",
    "#         print(\"......................\")\n",
    "#         print(values)\n",
    "        best = 0\n",
    "        idx = None\n",
    "        for val in values:\n",
    "#             val = values[i]\n",
    "            left = data[split_attribute_name] <= val\n",
    "            right = data[split_attribute_name] > val\n",
    "            counts = [len(data[split_attribute_name][left]), len(data[split_attribute_name][right])]\n",
    "            # print(counts, val)\n",
    "            Weighted_Entropy = (counts[0]/np.sum(counts))*entropy(data.where(data[split_attribute_name]<=val).dropna()[target_name], attributes[target_name], val) + (counts[1]/np.sum(counts))*entropy(data.where(data[split_attribute_name]>val).dropna()[target_name], attributes[target_name], val)\n",
    "            Information_Gain = total_entropy - Weighted_Entropy\n",
    "            if Information_Gain == 0.0:\n",
    "                continue\n",
    "            # print(Information_Gain)\n",
    "            Gain_Ratio = Information_Gain/entropy(data[split_attribute_name], attributes[split_attribute_name], val)\n",
    "#             print(entropy(data[split_attribute_name], attributes[split_attribute_name]), Gain_Ratio)\n",
    "#             print(\"Gain ratio\", Gain_Ratio)\n",
    "            if Gain_Ratio>=best:\n",
    "                best = Gain_Ratio\n",
    "                idx = val\n",
    "        # print(best, idx)\n",
    "        return best, idx\n",
    "\n",
    "def ID3(data,features,target_attribute_name=\"class\",parent_node_class = None):\n",
    "    if len(np.unique(data[target_attribute_name])) <= 1:\n",
    "        return np.unique(data[target_attribute_name])[0]\n",
    "\n",
    "#     elif len(data)==0 :\n",
    "#         return np.unique(originaldata[target_attribute_name])[np.argmax(np.unique(originaldata[target_attribute_name],return_counts=True)[1])]\n",
    "\n",
    "    elif len(features)==0 or len(data) == 0:\n",
    "        return parent_node_class\n",
    "\n",
    "    else:\n",
    "        parent_node_class = np.unique(data[target_attribute_name])[np.argmax(np.unique(data[target_attribute_name],return_counts=True)[1])]\n",
    "\n",
    "        item_values = []\n",
    "        split_point = []\n",
    "        for feature in features:\n",
    "            GR, point = InfoGainRatio(data,feature,target_attribute_name)\n",
    "            item_values.append(GR)\n",
    "            split_point.append(point)\n",
    "        best_feature_index = np.argmax(item_values)\n",
    "        best_feature = features[best_feature_index]\n",
    "\n",
    "        tree = {best_feature:{}}\n",
    "\n",
    "        features = [i for i in features if i != best_feature]\n",
    "        if attributes[best_feature] == 'category':\n",
    "            for value in np.unique(data[best_feature]):\n",
    "                value = value\n",
    "                sub_data = data.where(data[best_feature] == value).dropna()\n",
    "\n",
    "                subtree = ID3(sub_data,features,target_attribute_name,parent_node_class)\n",
    "                tree[best_feature][value] = subtree\n",
    "\n",
    "            return(tree)\n",
    "        else:\n",
    "            sub_data1 = data.where(data[best_feature]<=split_point[best_feature_index]).dropna()\n",
    "            sub_data2 = data.where(data[best_feature]>split_point[best_feature_index]).dropna()\n",
    "            subtree1 = ID3(sub_data1,features,target_attribute_name,parent_node_class)\n",
    "            tree[best_feature][split_point[best_feature_index] - 0.00001] = subtree1\n",
    "            subtree2 = ID3(sub_data2,features,target_attribute_name,parent_node_class)\n",
    "            tree[best_feature][split_point[best_feature_index] + 0.00001] = subtree2\n",
    "            return(tree)\n",
    "\n",
    "\n",
    "def predict(query,tree,default = 1):\n",
    "    if not isinstance(tree, dict):\n",
    "        return tree\n",
    "    att_name = list(tree.keys())[0]\n",
    "    if attributes[att_name] == 'category':\n",
    "        try:\n",
    "            result_tree = tree[att_name][query[att_name]]\n",
    "        except:\n",
    "            return default\n",
    "        result_tree = tree[att_name][query[att_name]]\n",
    "        return predict(query, result_tree)\n",
    "    else:\n",
    "        key_vals = list(tree[att_name].keys())\n",
    "        if abs(key_vals[0]-query[att_name]) < abs(key_vals[1]-query[att_name]):\n",
    "            result_tree = tree[att_name][key_vals[0]]\n",
    "        else:\n",
    "            result_tree = tree[att_name][key_vals[1]]\n",
    "        return predict(query, result_tree)\n",
    "\n",
    "def printDecisionTree(tree,level):\n",
    "    level += \" \"\n",
    "    for key,value in tree.items():\n",
    "        if isinstance(value, dict):\n",
    "            print(level+str(key)+\":\")\n",
    "            printDecisionTree(value, level)\n",
    "        else: \n",
    "            print(level+str(key)+\"-->\",value)    \n",
    "\n",
    "\n",
    "def test(data,tree):\n",
    "    queries = data.iloc[:,:-1].to_dict(orient = \"records\")\n",
    "    predictions = [predict(query,tree) for query in queries]\n",
    "    accuracy = accuracy_score(predictions, list(data[\"class\"]))\n",
    "    precision = precision_score(predictions, list(data[\"class\"]), average=\"macro\")\n",
    "    recall = recall_score(predictions, list(data[\"class\"]), average=\"macro\")\n",
    "    f1 = f1_score(predictions, list(data[\"class\"]), average=\"macro\")\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "attributes, dataset = getData(dataset_name='iris')\n",
    "training_data, testing_data = train_test_split(dataset, test_size = 0.2)\n",
    "\n",
    "tree = ID3(training_data,training_data,training_data.columns[:-1])\n",
    "accuracy, precision, recall, f1 = test(testing_data,tree)\n",
    "printDecisionTree(tree,\"\")\n",
    "\n",
    "\n",
    "print(\"accuracy \\t precision \\t recall \\t f1\")\n",
    "print(\"{:.2f}\".format(accuracy*100),\"%\\t\\t\", \n",
    "      \"{:.2f}\".format(precision*100),\"%\\t\",\n",
    "      \"{:.2f}\".format(recall*100),\"%\\t\", \n",
    "      \"{:.2f}\".format(f1*100),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset_name):\n",
    "    attribute_file_name = 'Data/'+dataset_name+\".attribute\"\n",
    "    dataset_file_name = 'Data/'+dataset_name+\".data\"\n",
    "    att = pd.read_csv(attribute_file_name, \n",
    "                      delim_whitespace=True,\n",
    "                     header = None)\n",
    "    attributes = {rows[0]:rows[1] for _,rows in att.iterrows()}\n",
    "    print(attributes)\n",
    "#     for _ , rows in att.iterrows():\n",
    "#         print(rows['c1'])\n",
    "get_data(dataset_name='iris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data2, testing_data2 = train_test_split(dataset, test_size = 0.2)\n",
    "\n",
    "training_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4+5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max([3,1,5,0,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
